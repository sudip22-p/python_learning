
======================================================
AI APIs and Pretrained Models - Implementation Guide
======================================================

This directory contains examples and implementation notes for using LLMs (Large Language Models) in Python through 3 major approaches:

1. OpenAI API (ChatGPT/GPT-4)
2. Hugging Face Transformers (Local/Cloud-hosted models)
3. LlamaCpp (Offline Meta LLaMA models via GGUF format)

It includes their real-world use cases, advantages/disadvantages, and setup instructions.

------------------------------------------------------
1. OpenAI LLM API (ChatGPT)
------------------------------------------------------

ğŸ“Œ Description:
OpenAI provides an API to use powerful models like:
- GPT-3.5-turbo (free tier)
- GPT-4 (pro tier)
- DALLÂ·E, Whisper, and more

These are hosted models â€” you send a request and get a response.

âœ… Use Cases:
- Chatbots (e.g., ChatGPT-style apps)
- Email/Resume generation tools
- Coding assistants (e.g., Replit Ghostwriter, Sourcegraph Cody)
- Language translators
- Customer support automation
- AI tutors / Q&A bots
- Summarizers and rewriters

ğŸ§  Real-World Usage:
- ChatGPT (chat.openai.com)
- Notion AI, Grammarly AI
- Replit Ghostwriter
- GitHub Copilot (internally uses Codex / GPT models)

âš™ï¸ Implementation (via `1_openai_llm.py`):
- Requires internet access
- You need an OpenAI API key (from https://platform.openai.com/)
- Uses Python package `openai`

ğŸ› ï¸ Setup:
1. Install dependencies:
   pip install openai python-dotenv

2. Create a `.env` file:
   OPENAI_API_KEY=your_key_here

3. Use the API in code (ChatCompletion):
   openai.ChatCompletion.create(
       model="gpt-3.5-turbo",
       messages=[{"role": "user", "content": "Hello!"}]
   )

ğŸ’¡ Tips:
- Use role="system" to control behavior.
- Stream responses for real-time apps.
- Token cost matters in production.

ğŸ“ Docs:
https://platform.openai.com/docs

------------------------------------------------------
2. Hugging Face Transformers
------------------------------------------------------

ğŸ“Œ Description:
The `transformers` library by Hugging Face gives access to 1000s of pretrained models for NLP, vision, and audio.

Models include:
- GPT-2, BERT, RoBERTa, DistilBERT, T5, Falcon, BLOOM, LLaMA, etc.

âœ… Use Cases:
- Sentiment Analysis
- Text generation
- Named Entity Recognition
- Machine Translation
- Question Answering
- Zero-shot classification

ğŸ§  Real-World Usage:
- Twitter AI moderation
- Spam filtering systems
- Translation tools
- Text summarization (e.g., news)

âš™ï¸ Implementation (via `2_huggingface_transformers.py`):
- You can run models locally or access hosted ones via `pipeline`
- Default models load from the Hugging Face hub

ğŸ› ï¸ Setup:
1. Install:
   pip install transformers

2. Optional (for performance):
   pip install torch

ğŸ“ Example:
from transformers import pipeline
gen = pipeline("text-generation", model="gpt2")
print(gen("Once upon a time,", max_length=20))

ğŸ”¥ Pros:
- No API keys needed
- Large community and documentation
- Offline model usage possible

âš ï¸ Cons:
- Requires good hardware for big models
- Some models are slow without GPU

ğŸ“ Docs:
https://huggingface.co/docs/transformers/index

------------------------------------------------------
3. LlamaCpp â€“ Offline LLaMA Models (skipped for now since large download required ...)
------------------------------------------------------

ğŸ“Œ Description:
LlamaCpp lets you run Meta's LLaMA models locally (fully offline) using GGUF files. It's optimized for CPU/GPU via C++ backend.

Popular for:
- Privacy-first AI apps
- Chatbots that work without internet
- Lightweight AI on edge devices

ğŸ§  Real-World Usage:
- Local private ChatGPT alternatives
- Raspberry Pi AI assistants
- Secure enterprise internal tools

âœ… Benefits:
- 100% offline (no API needed)
- Open source
- Works with 7B+, 13B, 70B parameter models (depends on hardware)

ğŸ› ï¸ Setup:

1. Install:
pip install llama-cpp-python

2. Download a GGUF model:
From Hugging Face:  
https://huggingface.co/TheBloke

Example:  
TheBloke/Llama-2-7B-Chat-GGUF

3. Code Example:
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-2-7b-chat.gguf")
res = llm("What is Python?", max_tokens=100)
print(res["choices"][0]["text"])

ğŸ’¡ Tips:
- You need at least 6-8 GB RAM for 7B models
- Use quantized models (e.g., Q4_0 or Q4_K) for less memory

âš ï¸ Cons:
- Large model downloads (2â€“8 GB+)
- Needs good CPU/GPU
- Manual setup of model paths

ğŸ“ Docs:
https://github.com/abetlen/llama-cpp-python

------------------------------------------------------
Summary Table
------------------------------------------------------

| Feature            | OpenAI API     | HF Transformers | LlamaCpp         |
|--------------------|----------------|------------------|------------------|
| Internet required  | âœ… Yes         | âŒ (Optional)     | âŒ No            |
| Key needed         | âœ… Yes         | âŒ No             | âŒ No            |
| Hosted by          | OpenAI Cloud   | Hugging Face Hub | Your own device  |
| Ease of setup      | âœ… Easy        | âœ… Easy           | âš ï¸ Medium/Hard   |
| Cost               | ğŸ§¾ Paid (API)  | Free              | Free             |
| Runs locally       | âŒ No          | âœ… Yes            | âœ… Yes           |
| Real-time usage    | âœ… Very fast   | âš ï¸ Depends         | âš ï¸ Depends       |
| Privacy            | âŒ External    | âœ…/âš ï¸ Hybrid        | âœ… Full Offline   |
| Flexibility        | Limited        | High              | High             |

------------------------------------------------------
Final Thoughts
------------------------------------------------------

If you're building:

- ğŸ’» A web app â†’ Start with OpenAI API
- ğŸ§ª A research tool â†’ Use Hugging Face
- ğŸ” A private offline chatbot â†’ Go with LlamaCpp

======================================================
