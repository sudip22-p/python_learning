
======================================================
AI APIs and Pretrained Models - Implementation Guide
======================================================

This directory contains examples and implementation notes for using LLMs (Large Language Models) in Python through 3 major approaches:

1. OpenAI API (ChatGPT/GPT-4)
2. Hugging Face Transformers (Local/Cloud-hosted models)
3. LlamaCpp (Offline Meta LLaMA models via GGUF format)

It includes their real-world use cases, advantages/disadvantages, and setup instructions.

------------------------------------------------------
1. OpenAI LLM API (ChatGPT)
------------------------------------------------------

📌 Description:
OpenAI provides an API to use powerful models like:
- GPT-3.5-turbo (free tier)
- GPT-4 (pro tier)
- DALL·E, Whisper, and more

These are hosted models — you send a request and get a response.

✅ Use Cases:
- Chatbots (e.g., ChatGPT-style apps)
- Email/Resume generation tools
- Coding assistants (e.g., Replit Ghostwriter, Sourcegraph Cody)
- Language translators
- Customer support automation
- AI tutors / Q&A bots
- Summarizers and rewriters

🧠 Real-World Usage:
- ChatGPT (chat.openai.com)
- Notion AI, Grammarly AI
- Replit Ghostwriter
- GitHub Copilot (internally uses Codex / GPT models)

⚙️ Implementation (via `1_openai_llm.py`):
- Requires internet access
- You need an OpenAI API key (from https://platform.openai.com/)
- Uses Python package `openai`

🛠️ Setup:
1. Install dependencies:
   pip install openai python-dotenv

2. Create a `.env` file:
   OPENAI_API_KEY=your_key_here

3. Use the API in code (ChatCompletion):
   openai.ChatCompletion.create(
       model="gpt-3.5-turbo",
       messages=[{"role": "user", "content": "Hello!"}]
   )

💡 Tips:
- Use role="system" to control behavior.
- Stream responses for real-time apps.
- Token cost matters in production.

📎 Docs:
https://platform.openai.com/docs

------------------------------------------------------
2. Hugging Face Transformers
------------------------------------------------------

📌 Description:
The `transformers` library by Hugging Face gives access to 1000s of pretrained models for NLP, vision, and audio.

Models include:
- GPT-2, BERT, RoBERTa, DistilBERT, T5, Falcon, BLOOM, LLaMA, etc.

✅ Use Cases:
- Sentiment Analysis
- Text generation
- Named Entity Recognition
- Machine Translation
- Question Answering
- Zero-shot classification

🧠 Real-World Usage:
- Twitter AI moderation
- Spam filtering systems
- Translation tools
- Text summarization (e.g., news)

⚙️ Implementation (via `2_huggingface_transformers.py`):
- You can run models locally or access hosted ones via `pipeline`
- Default models load from the Hugging Face hub

🛠️ Setup:
1. Install:
   pip install transformers

2. Optional (for performance):
   pip install torch

📝 Example:
from transformers import pipeline
gen = pipeline("text-generation", model="gpt2")
print(gen("Once upon a time,", max_length=20))

🔥 Pros:
- No API keys needed
- Large community and documentation
- Offline model usage possible

⚠️ Cons:
- Requires good hardware for big models
- Some models are slow without GPU

📎 Docs:
https://huggingface.co/docs/transformers/index

------------------------------------------------------
3. LlamaCpp – Offline LLaMA Models (skipped for now since large download required ...)
------------------------------------------------------

📌 Description:
LlamaCpp lets you run Meta's LLaMA models locally (fully offline) using GGUF files. It's optimized for CPU/GPU via C++ backend.

Popular for:
- Privacy-first AI apps
- Chatbots that work without internet
- Lightweight AI on edge devices

🧠 Real-World Usage:
- Local private ChatGPT alternatives
- Raspberry Pi AI assistants
- Secure enterprise internal tools

✅ Benefits:
- 100% offline (no API needed)
- Open source
- Works with 7B+, 13B, 70B parameter models (depends on hardware)

🛠️ Setup:

1. Install:
pip install llama-cpp-python

2. Download a GGUF model:
From Hugging Face:  
https://huggingface.co/TheBloke

Example:  
TheBloke/Llama-2-7B-Chat-GGUF

3. Code Example:
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-2-7b-chat.gguf")
res = llm("What is Python?", max_tokens=100)
print(res["choices"][0]["text"])

💡 Tips:
- You need at least 6-8 GB RAM for 7B models
- Use quantized models (e.g., Q4_0 or Q4_K) for less memory

⚠️ Cons:
- Large model downloads (2–8 GB+)
- Needs good CPU/GPU
- Manual setup of model paths

📎 Docs:
https://github.com/abetlen/llama-cpp-python

------------------------------------------------------
Summary Table
------------------------------------------------------

| Feature            | OpenAI API     | HF Transformers | LlamaCpp         |
|--------------------|----------------|------------------|------------------|
| Internet required  | ✅ Yes         | ❌ (Optional)     | ❌ No            |
| Key needed         | ✅ Yes         | ❌ No             | ❌ No            |
| Hosted by          | OpenAI Cloud   | Hugging Face Hub | Your own device  |
| Ease of setup      | ✅ Easy        | ✅ Easy           | ⚠️ Medium/Hard   |
| Cost               | 🧾 Paid (API)  | Free              | Free             |
| Runs locally       | ❌ No          | ✅ Yes            | ✅ Yes           |
| Real-time usage    | ✅ Very fast   | ⚠️ Depends         | ⚠️ Depends       |
| Privacy            | ❌ External    | ✅/⚠️ Hybrid        | ✅ Full Offline   |
| Flexibility        | Limited        | High              | High             |

------------------------------------------------------
Final Thoughts
------------------------------------------------------

If you're building:

- 💻 A web app → Start with OpenAI API
- 🧪 A research tool → Use Hugging Face
- 🔐 A private offline chatbot → Go with LlamaCpp

======================================================
